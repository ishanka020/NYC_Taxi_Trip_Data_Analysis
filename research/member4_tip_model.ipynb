{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-09T09:16:21.296893Z",
     "start_time": "2025-06-09T09:15:36.039479Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create SparkSession in local mode\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FareTipModeling\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T09:16:24.200849Z",
     "start_time": "2025-06-09T09:16:24.169480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "folder_path = Path(r\"D:\\L4S2\\Big Data\\assignment\\NYC_Taxi_Trip_Data_Analysis\\data\\cleaned\")\n",
    "\n",
    "parquet_files = sorted(folder_path.glob(\"*.parquet\"))\n",
    "print(f\"Found {len(parquet_files)} parquet files:\")\n",
    "for f in parquet_files:\n",
    "    print(f.name)\n"
   ],
   "id": "2289b82b1c43e618",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 parquet files:\n",
      "part-00000-c267671b-1832-48c2-97b7-10a9827a836f-c000.snappy.parquet\n",
      "part-00001-c267671b-1832-48c2-97b7-10a9827a836f-c000.snappy.parquet\n",
      "part-00002-c267671b-1832-48c2-97b7-10a9827a836f-c000.snappy.parquet\n",
      "part-00003-c267671b-1832-48c2-97b7-10a9827a836f-c000.snappy.parquet\n",
      "part-00004-c267671b-1832-48c2-97b7-10a9827a836f-c000.snappy.parquet\n",
      "part-00005-c267671b-1832-48c2-97b7-10a9827a836f-c000.snappy.parquet\n",
      "part-00006-c267671b-1832-48c2-97b7-10a9827a836f-c000.snappy.parquet\n",
      "part-00007-c267671b-1832-48c2-97b7-10a9827a836f-c000.snappy.parquet\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T09:16:30.483392Z",
     "start_time": "2025-06-09T09:16:30.476156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_valid_fares_tips(df):\n",
    "    filtered_df = df.filter((df.fare_amount > 0) & (df.tip_amount >= 0))\n",
    "    print(f\"Filtered from {df.count()} rows to {filtered_df.count()} rows with valid fare and tip amounts\")\n",
    "    return filtered_df\n"
   ],
   "id": "a1661c6cb8a64f0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T09:16:52.329792Z",
     "start_time": "2025-06-09T09:16:33.490434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file_path in parquet_files:\n",
    "    print(f\"\\nProcessing file: {file_path.name}\")\n",
    "    df = spark.read.parquet(str(file_path))\n",
    "    \n",
    "    # Run your filter\n",
    "    filtered_df = filter_valid_fares_tips(df)\n",
    "    \n",
    "    # You can add more processing steps here (e.g. modeling)\n"
   ],
   "id": "64a779d5a3a5c1a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: part-00000-c267671b-1832-48c2-97b7-10a9827a836f-c000.snappy.parquet\n",
      "Filtered from 3918134 rows to 3918134 rows with valid fare and tip amounts\n",
      "\n",
      "Processing file: part-00001-c267671b-1832-48c2-97b7-10a9827a836f-c000.snappy.parquet\n",
      "Filtered from 3933455 rows to 3933455 rows with valid fare and tip amounts\n",
      "\n",
      "Processing file: part-00002-c267671b-1832-48c2-97b7-10a9827a836f-c000.snappy.parquet\n",
      "Filtered from 3928190 rows to 3928190 rows with valid fare and tip amounts\n",
      "\n",
      "Processing file: part-00003-c267671b-1832-48c2-97b7-10a9827a836f-c000.snappy.parquet\n",
      "Filtered from 3936382 rows to 3936382 rows with valid fare and tip amounts\n",
      "\n",
      "Processing file: part-00004-c267671b-1832-48c2-97b7-10a9827a836f-c000.snappy.parquet\n",
      "Filtered from 3930808 rows to 3930808 rows with valid fare and tip amounts\n",
      "\n",
      "Processing file: part-00005-c267671b-1832-48c2-97b7-10a9827a836f-c000.snappy.parquet\n",
      "Filtered from 5205187 rows to 5205187 rows with valid fare and tip amounts\n",
      "\n",
      "Processing file: part-00006-c267671b-1832-48c2-97b7-10a9827a836f-c000.snappy.parquet\n",
      "Filtered from 3411161 rows to 3411161 rows with valid fare and tip amounts\n",
      "\n",
      "Processing file: part-00007-c267671b-1832-48c2-97b7-10a9827a836f-c000.snappy.parquet\n",
      "Filtered from 3886616 rows to 3886616 rows with valid fare and tip amounts\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T09:17:09.422303Z",
     "start_time": "2025-06-09T09:17:09.264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = spark.read.parquet(str(parquet_files[0]))\n",
    "print(df.columns)\n"
   ],
   "id": "44627bfecc13821",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'Airport_fee']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T09:17:15.132266Z",
     "start_time": "2025-06-09T09:17:15.122594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#calculate the total size of the data set\n",
    "from pathlib import Path\n",
    "\n",
    "folder_path = Path(r\"D:\\L4S2\\Big Data\\assignment\\NYC_Taxi_Trip_Data_Analysis\\data\\cleaned\")\n",
    "\n",
    "total_size_bytes = sum(f.stat().st_size for f in folder_path.glob(\"*.parquet\"))\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"ðŸ“¦ Total dataset size: {total_size_mb:.2f} MB\")\n"
   ],
   "id": "f46b5b11f193d675",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Total dataset size: 624.47 MB\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T09:17:24.306468Z",
     "start_time": "2025-06-09T09:17:23.694195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ðŸ“„ Load and combine all Parquet files into one DataFrame\n",
    "df_all = spark.read.parquet(*[str(p) for p in parquet_files])\n",
    "\n",
    "# ðŸŽ¯ Select 50 sample values from the tip_amount column\n",
    "print(\"ðŸŽ¯ Sample 50 values from 'tip_amount' column:\")\n",
    "df_all.select(\"tip_amount\").limit(50).show(50, truncate=False)\n"
   ],
   "id": "16fb4f0094fb2f0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Sample 50 values from 'tip_amount' column:\n",
      "+----------+\n",
      "|tip_amount|\n",
      "+----------+\n",
      "|0.0       |\n",
      "|1.0       |\n",
      "|1.0       |\n",
      "|0.0       |\n",
      "|3.0       |\n",
      "|3.42      |\n",
      "|2.08      |\n",
      "|2.72      |\n",
      "|4.0       |\n",
      "|2.0       |\n",
      "|0.1       |\n",
      "|3.84      |\n",
      "|0.0       |\n",
      "|6.64      |\n",
      "|3.84      |\n",
      "|2.44      |\n",
      "|0.0       |\n",
      "|2.13      |\n",
      "|2.0       |\n",
      "|2.44      |\n",
      "|2.08      |\n",
      "|3.28      |\n",
      "|0.0       |\n",
      "|4.68      |\n",
      "|5.0       |\n",
      "|2.0       |\n",
      "|0.0       |\n",
      "|4.26      |\n",
      "|3.4       |\n",
      "|4.26      |\n",
      "|1.85      |\n",
      "|0.0       |\n",
      "|0.0       |\n",
      "|16.15     |\n",
      "|2.0       |\n",
      "|2.1       |\n",
      "|2.0       |\n",
      "|4.3       |\n",
      "|2.0       |\n",
      "|0.0       |\n",
      "|0.0       |\n",
      "|5.8       |\n",
      "|4.1       |\n",
      "|1.0       |\n",
      "|1.0       |\n",
      "|3.4       |\n",
      "|0.0       |\n",
      "|3.15      |\n",
      "|2.45      |\n",
      "|7.6       |\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T09:25:40.017914Z",
     "start_time": "2025-06-09T09:25:39.994970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Drop 'total_amount' if it exists to avoid data leakage\n",
    "if 'total_amount' in filtered_df.columns:\n",
    "    filtered_df = filtered_df.drop('total_amount')\n",
    "\n",
    "# Select only numeric feature columns and drop target\n",
    "feature_cols = [col for col in filtered_df.columns if col != 'tip_amount']\n",
    "target_col = 'tip_amount'\n"
   ],
   "id": "c14ee5c6b4e0a271",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Feature Engineering I\n",
   "id": "1fcc90cc9bf8736d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T09:38:29.008346Z",
     "start_time": "2025-06-09T09:38:28.282789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import hour, dayofweek, unix_timestamp, col\n",
    "\n",
    "# Add time-based features\n",
    "engineered_df = filtered_df \\\n",
    "    .withColumn(\"pickup_hour\", hour(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"pickup_dayofweek\", dayofweek(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"trip_duration_minutes\", \n",
    "                (unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"tpep_pickup_datetime\"))) / 60)\n",
    "\n",
    "# Optional: Drop original timestamp fields\n",
    "engineered_df = engineered_df.drop(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\")\n"
   ],
   "id": "fe011d35b4dd4c3",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T09:38:52.544488Z",
     "start_time": "2025-06-09T09:38:52.372174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "engineered_df = engineered_df.withColumn(\"store_and_fwd_flag_numeric\",\n",
    "                                         when(col(\"store_and_fwd_flag\") == \"Y\", 1).otherwise(0))\n",
    "\n",
    "# Drop original string column\n",
    "engineered_df = engineered_df.drop(\"store_and_fwd_flag\")\n"
   ],
   "id": "d20535c2aee8ddb",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T09:39:10.069746Z",
     "start_time": "2025-06-09T09:39:09.887713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "numeric_cols = [f.name for f in engineered_df.schema.fields \n",
    "                if isinstance(f.dataType, NumericType) and f.name != \"tip_amount\"]\n",
    "\n",
    "print(\"Final feature columns:\")\n",
    "print(numeric_cols)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(engineered_df).select(\"features\", \"tip_amount\")\n"
   ],
   "id": "72ce93c3c4ea6bfc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature columns:\n",
      "['VendorID', 'passenger_count', 'trip_distance', 'RatecodeID', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', 'Airport_fee', 'pickup_hour', 'pickup_dayofweek', 'trip_duration_minutes', 'store_and_fwd_flag_numeric']\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T09:40:40.526771Z",
     "start_time": "2025-06-09T09:39:30.072562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data, test_data = assembled_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"tip_amount\")\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"tip_amount\", predictionCol=\"prediction\", metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"New RMSE with engineered features = {rmse:.2f}\")\n"
   ],
   "id": "6edefa0f91e1cc48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New RMSE with engineered features = 2.57\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Feature Engineering II",
   "id": "cdf412b6ea6a6337"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T09:26:52.318196Z",
     "start_time": "2025-06-09T09:26:52.016174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "# Get only numeric columns (exclude timestamps, strings, etc.)\n",
    "numeric_cols = [f.name for f in filtered_df.schema.fields\n",
    "                if isinstance(f.dataType, NumericType) and f.name != 'tip_amount']\n",
    "\n",
    "print(\"Using these numeric columns for features:\")\n",
    "print(numeric_cols)\n",
    "\n",
    "# Assemble features into 'features' column\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol='features')\n",
    "assembled_df = assembler.transform(filtered_df).select('features', 'tip_amount')\n"
   ],
   "id": "d6c8eb46fce47340",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using these numeric columns for features:\n",
      "['VendorID', 'passenger_count', 'trip_distance', 'RatecodeID', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', 'Airport_fee']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T09:28:41.302651Z",
     "start_time": "2025-06-09T09:27:15.834484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data, test_data = assembled_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training rows: {train_data.count()}, Testing rows: {test_data.count()}\")\n"
   ],
   "id": "eeb916b752c1d8f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rows: 3108808, Testing rows: 777808\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T09:30:38.528257Z",
     "start_time": "2025-06-09T09:29:29.276657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol='features', labelCol='tip_amount')\n",
    "lr_model = lr.fit(train_data)\n"
   ],
   "id": "6f34b97c84b2b3ff",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T09:31:10.781958Z",
     "start_time": "2025-06-09T09:30:51.685165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions = lr_model.transform(test_data)\n",
    "predictions.select(\"features\", \"tip_amount\", \"prediction\").show(5)\n"
   ],
   "id": "6104550d77fdce6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------------------+\n",
      "|            features|tip_amount|         prediction|\n",
      "+--------------------+----------+-------------------+\n",
      "|[1.0,1.0,0.1,1.0,...|       0.0|-5.4452204766350185|\n",
      "|[1.0,1.0,0.1,1.0,...|       0.0| -2.112436338648564|\n",
      "|[1.0,1.0,0.1,1.0,...|       0.0| -2.020170053631144|\n",
      "|[1.0,1.0,0.1,1.0,...|       0.0| -4.227821951385367|\n",
      "|[1.0,1.0,0.1,1.0,...|       0.0| -7.086102416301555|\n",
      "+--------------------+----------+-------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T09:33:31.930561Z",
     "start_time": "2025-06-09T09:33:10.839874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol='tip_amount', predictionCol='prediction', metricName='rmse')\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse:.2f}\")\n"
   ],
   "id": "26edc5edc74e53ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2.57\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
